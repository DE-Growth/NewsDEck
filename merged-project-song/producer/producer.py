import os
import json
import time
import requests
import re
from kafka import KafkaProducer
from kafka.errors import NoBrokersAvailable
from dotenv import load_dotenv
from datetime import datetime
import logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IntegratedNewsProducer:
    """
    í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°
    - seoyunjangì˜ í´ë˜ìŠ¤ ê¸°ë°˜ êµ¬ì¡° ì±„íƒ
    - song-testì˜ HTML ì •ë¦¬ ê¸°ëŠ¥ê³¼ ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
    """
    
    def __init__(self):
        # ë„¤ì´ë²„ API ì„¤ì •
        self.client_id = os.getenv('NAVER_CLIENT_ID')
        self.client_secret = os.getenv('NAVER_CLIENT_SECRET')
        
        # Kafka ì„¤ì • (ë¡œì»¬/ë„ì»¤ í™˜ê²½ ìë™ ê°ì§€)
        self.kafka_servers = self._get_kafka_servers()
        self.topic = os.getenv('KAFKA_TOPIC', 'news-topic')
        
        # ìˆ˜ì§‘ ì„¤ì •
        self.keywords = self._get_keywords()
        self.collect_interval = int(os.getenv('COLLECT_INTERVAL', 600))  # 10ë¶„ (600ì´ˆ)
        self.articles_per_keyword = int(os.getenv('ARTICLES_PER_KEYWORD', 10))
        
        # API URL
        self.api_url = "https://openapi.naver.com/v1/search/news.json"
        
        # Kafka Producer ì´ˆê¸°í™”
        self.producer = self._create_kafka_producer()
        
        logger.info(f"í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"Kafka: {self.kafka_servers}, Topic: {self.topic}")
        logger.info(f"í‚¤ì›Œë“œ: {self.keywords}")
        
    def _get_kafka_servers(self):
        """Kafka ì„œë²„ ì£¼ì†Œ ê²°ì • (í™˜ê²½ ìë™ ê°ì§€)"""
        kafka_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS')
        
        if kafka_servers:
            return kafka_servers
            
        # ìë™ ê°ì§€: Docker í™˜ê²½ vs ë¡œì»¬ í™˜ê²½
        if os.path.exists('/.dockerenv'):
            # Docker í™˜ê²½
            return 'kafka:9092'
        else:
            # ë¡œì»¬ í™˜ê²½
            return 'localhost:9093'
    
    def _get_keywords(self):
        """ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        keywords_str = os.getenv('COLLECT_KEYWORDS', 'IT,ì£¼ì‹,AI,ë°˜ë„ì²´,ê²½ì œ')
        return [keyword.strip() for keyword in keywords_str.split(',')]
    
    def _create_kafka_producer(self):
        """Kafka Producer ìƒì„± (ì—°ê²° ì¬ì‹œë„ í¬í•¨)"""
        max_retries = 5
        retry_delay = 10
        
        for attempt in range(max_retries):
            try:
                producer = KafkaProducer(
                    bootstrap_servers=[self.kafka_servers],
                    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8'),
                    acks='all',
                    retries=3,
                    batch_size=16384,
                    linger_ms=100
                )
                
                logger.info(f"Kafka Producer ì—°ê²° ì„±ê³µ: {self.kafka_servers}")
                return producer
                
            except NoBrokersAvailable:
                logger.warning(f"Kafka ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}). {retry_delay}ì´ˆ í›„ ì¬ì‹œë„...")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                else:
                    logger.error("Kafka ì—°ê²° ìµœì¢… ì‹¤íŒ¨")
                    raise
    
    def clean_html(self, text):
        """HTML íƒœê·¸ì™€ ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±° (song-test ê¸°ë°˜)"""
        if not text:
            return ""
            
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<.*?>", "", text)
        
        # HTML ì—”í‹°í‹° ë³€í™˜
        text = text.replace("&quot;", '"')
        text = text.replace("&lt;", "<")
        text = text.replace("&gt;", ">")
        text = text.replace("&amp;", "&")
        text = text.replace("&#39;", "'")
        text = text.replace("&nbsp;", " ")
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def search_news(self, query, display=None):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬)"""
        if not self.client_id or not self.client_secret:
            logger.error("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        display = display or self.articles_per_keyword
        
        headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'IntegratedNewsProducer/1.0'
        }
        
        params = {
            'query': query,
            'display': display,
            'sort': 'date',
            'start': 1
        }
        
        try:
            logger.info(f"'{query}' ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘... (ìµœëŒ€ {display}ê±´)")
            
            response = requests.get(
                self.api_url, 
                headers=headers, 
                params=params,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            items = result.get('items', [])
            
            logger.info(f"'{query}' ê²€ìƒ‰ ì™„ë£Œ: {len(items)}ê±´ ìˆ˜ì§‘")
            return items
            
        except requests.exceptions.RequestException as e:
            logger.error(f"'{query}' API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"'{query}' JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def send_to_kafka(self, data):
        """Kafkaë¡œ ë°ì´í„° ì „ì†¡ (ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬)"""
        try:
            future = self.producer.send(self.topic, value=data)
            record_metadata = future.get(timeout=10)
            
            logger.debug(f"Kafka ì „ì†¡ ì„±ê³µ: {record_metadata.topic}[{record_metadata.partition}] offset {record_metadata.offset}")
            return True
            
        except Exception as e:
            logger.error(f"Kafka ì „ì†¡ ì‹¤íŒ¨: {e}")
            return False
    
    def process_article(self, item, keyword):
        """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ì²˜ë¦¬ ë° ì •ì œ"""
        try:
            # ë°ì´í„° ì •ì œ
            title = self.clean_html(item.get('title', ''))
            description = self.clean_html(item.get('description', ''))
            
            # í•„ìˆ˜ ë°ì´í„° ê²€ì¦
            if not title or not item.get('link'):
                logger.warning(f"í•„ìˆ˜ ë°ì´í„° ëˆ„ë½: title={bool(title)}, link={bool(item.get('link'))}")
                return None
            
            # í†µí•© ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡°
            news_data = {
                'keyword': keyword,
                'title': title[:500],  # ê¸¸ì´ ì œí•œ
                'link': item.get('link', ''),
                'originallink': item.get('originallink', ''),
                'description': description[:1000],  # ê¸¸ì´ ì œí•œ
                'pubDate': item.get('pubDate', ''),
                'pub_date': item.get('pubDate', ''),  # ì–‘ìª½ í˜•ì‹ ì§€ì›
                'collected_at': datetime.now().isoformat(),
                'source': 'naver_news_api',
                'producer_version': '1.0'
            }
            
            return news_data
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def collect_news_by_keyword(self, keyword):
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # API í˜¸ì¶œ
            articles = self.search_news(keyword)
            if not articles:
                return 0
            
            success_count = 0
            
            for article in articles:
                # ê¸°ì‚¬ ë°ì´í„° ì²˜ë¦¬
                news_data = self.process_article(article, keyword)
                if not news_data:
                    continue
                
                # Kafkaë¡œ ì „ì†¡
                if self.send_to_kafka(news_data):
                    success_count += 1
                    logger.info(f"[{keyword}] '{news_data['title'][:50]}...' ì „ì†¡ ì™„ë£Œ")
                else:
                    logger.error(f"[{keyword}] '{news_data['title'][:50]}...' ì „ì†¡ ì‹¤íŒ¨")
            
            # Kafka flush (ì¦‰ì‹œ ì „ì†¡ ë³´ì¥)
            self.producer.flush()
            
            logger.info(f"[{keyword}] ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(articles)}ê±´ ì„±ê³µ")
            return success_count
            
        except Exception as e:
            logger.error(f"[{keyword}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return 0
    
    def run_once(self):
        """í•œ ë²ˆì˜ ìˆ˜ì§‘ ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("=== ë‰´ìŠ¤ ìˆ˜ì§‘ ì‚¬ì´í´ ì‹œì‘ ===")
        start_time = time.time()
        
        total_collected = 0
        
        for keyword in self.keywords:
            try:
                collected = self.collect_news_by_keyword(keyword)
                total_collected += collected
                
                # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (í‚¤ì›Œë“œ ê°„ 1ì´ˆ ëŒ€ê¸°)
                if keyword != self.keywords[-1]:  # ë§ˆì§€ë§‰ í‚¤ì›Œë“œê°€ ì•„ë‹ˆë©´
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"í‚¤ì›Œë“œ '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        elapsed_time = time.time() - start_time
        logger.info(f"=== ìˆ˜ì§‘ ì‚¬ì´í´ ì™„ë£Œ: ì´ {total_collected}ê±´ ìˆ˜ì§‘ (ì†Œìš”ì‹œê°„: {elapsed_time:.1f}ì´ˆ) ===")
        
        return total_collected
    
    def run_continuous(self):
        """ì§€ì†ì ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰"""
        logger.info("=== í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘ ===")
        logger.info(f"ìˆ˜ì§‘ ì£¼ê¸°: {self.collect_interval}ì´ˆ")
        logger.info("Ctrl+Cë¡œ ì¢…ë£Œ")
        
        cycle_count = 0
        
        try:
            while True:
                cycle_count += 1
                logger.info(f"\n--- ìˆ˜ì§‘ ì‚¬ì´í´ #{cycle_count} ---")
                
                collected = self.run_once()
                
                if collected > 0:
                    logger.info(f"âœ… ì‚¬ì´í´ #{cycle_count} ì„±ê³µ: {collected}ê±´ ìˆ˜ì§‘")
                else:
                    logger.warning(f"âš ï¸ ì‚¬ì´í´ #{cycle_count} ì‹¤íŒ¨: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
                
                logger.info(f"â° ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ {self.collect_interval}ì´ˆ ëŒ€ê¸°...")
                time.sleep(self.collect_interval)
                
        except KeyboardInterrupt:
            logger.info("\nğŸ›‘ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘ê¸° ì¢…ë£Œ")
        except Exception as e:
            logger.error(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ìˆ˜ì§‘ê¸° ì¢…ë£Œ: {e}")
        finally:
            self.close()
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if self.producer:
                self.producer.flush()
                self.producer.close()
                logger.info("Kafka Producer ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        producer = IntegratedNewsProducer()
        
        # í™˜ê²½ ë³€ìˆ˜ì— ë”°ë¼ ì‹¤í–‰ ëª¨ë“œ ê²°ì •
        run_mode = os.getenv('RUN_MODE', 'continuous')
        
        if run_mode == 'once':
            # í•œ ë²ˆë§Œ ì‹¤í–‰
            producer.run_once()
        else:
            # ì§€ì†ì  ì‹¤í–‰
            producer.run_continuous()
            
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹¤íŒ¨: {e}")
        exit(1)

if __name__ == "__main__":
    main()
