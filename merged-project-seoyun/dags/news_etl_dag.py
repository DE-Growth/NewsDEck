from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
import json
import requests
import time
import psycopg2
from kafka import KafkaProducer, KafkaConsumer
from datetime import datetime as dt
import pytz

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

# ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'seoyunjang',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# DAG ì •ì˜
dag = DAG(
    'integrated_news_pipeline',
    default_args=default_args,
    description='í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ íŒŒì´í”„ë¼ì¸',
    schedule_interval=timedelta(minutes=30),
    catchup=False,
    tags=['news', 'integrated', 'kafka', 'producer', 'consumer']
)

def collect_and_store_news():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ Kafka ì „ì†¡ â†’ PostgreSQL ì €ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬"""
    
    print("ğŸš€ í†µí•© ë‰´ìŠ¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    
    # ========================
    # 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ (Producer ì—­í• )
    # ========================
    print("ğŸ“° 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ë„¤ì´ë²„ API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ëŒ€ì‹  ì„¤ì •ê°’ ì‚¬ìš©)
    client_id = "nHALZLzPSxgWWrVlQfp6"  # ì‹¤ì œ API í‚¤
    client_secret = "T73FiHjlpT"  # ì‹¤ì œ ì‹œí¬ë¦¿
    
    # Kafka Producer ì„¤ì •
    producer = KafkaProducer(
        bootstrap_servers=['kafka:9092'],
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
    )
    
    keywords = ['IT', 'AI', 'ë¸”ë¡ì²´ì¸']  # ìˆ˜ì§‘í•  í‚¤ì›Œë“œë“¤
    collected_news = []
    
    for keyword in keywords:
        print(f"ğŸ” '{keyword}' ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            'X-Naver-Client-Id': client_id,
            'X-Naver-Client-Secret': client_secret
        }
        params = {
            'query': keyword,
            'display': 5,
            'sort': 'date'
        }
        
        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            result = response.json()
            
            if result and result.get('items'):
                for item in result['items']:
                    # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œê°„ ì„¤ì •
                    kst_now = datetime.now(KST)
                    
                    news_data = {
                        'title': item.get('title', '').replace('<b>', '').replace('</b>', ''),
                        'description': item.get('description', '').replace('<b>', '').replace('</b>', ''),
                        'link': f"{item.get('link', '')}?collected_at={int(kst_now.timestamp())}",  # í•œêµ­ ì‹œê°„ íƒ€ì„ìŠ¤íƒ¬í”„
                        'pub_date': item.get('pubDate', ''),
                        'keyword': keyword,
                        'collected_at': kst_now.isoformat()  # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì„¤ì •
                    }
                    
                    # Kafkaë¡œ ì „ì†¡
                    try:
                        future = producer.send('news-topic', value=news_data)
                        record_metadata = future.get(timeout=10)
                        collected_news.append(news_data)
                        print(f"  âœ… Kafka ì „ì†¡: {news_data['title'][:50]}...")
                    except Exception as e:
                        print(f"  âŒ Kafka ì „ì†¡ ì‹¤íŒ¨: {e}")
                
                time.sleep(1)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
        
        except Exception as e:
            print(f"âŒ '{keyword}' API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
    
    producer.close()
    print(f"ğŸ“¤ ì´ {len(collected_news)}ê°œ ë‰´ìŠ¤ë¥¼ Kafkaë¡œ ì „ì†¡ ì™„ë£Œ!")
    
    # ========================
    # 2ë‹¨ê³„: ì ì‹œ ëŒ€ê¸° (Kafka ë©”ì‹œì§€ ì•ˆì •í™”)
    # ========================
    print("â³ 2ë‹¨ê³„: Kafka ë©”ì‹œì§€ ì•ˆì •í™” ëŒ€ê¸°...")
    time.sleep(5)
    
    # ========================
    # 3ë‹¨ê³„: Kafkaì—ì„œ ë°ì´í„° ì½ì–´ì„œ PostgreSQL ì €ì¥ (Consumer ì—­í• )
    # ========================
    print("ğŸ’¾ 3ë‹¨ê³„: PostgreSQL ì €ì¥ ì‹œì‘...")
    
    # Kafka Consumer ì„¤ì •
    consumer = KafkaConsumer(
        'news-topic',
        bootstrap_servers=['kafka:9092'],
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        group_id=f'integrated-consumer-{int(datetime.now(KST).timestamp())}',  # í•œêµ­ ì‹œê°„ ê¸°ë°˜ ìœ ë‹ˆí¬ ê·¸ë£¹ ID
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        consumer_timeout_ms=15000  # 15ì´ˆ íƒ€ì„ì•„ì›ƒ
    )
    
    messages_processed = 0
    try:
        for message in consumer:
            try:
                news_data = message.value
                if not news_data.get('title'):
                    continue
                
                # PostgreSQL ì§ì ‘ ì—°ê²°
                conn = psycopg2.connect(
                    host='postgres',
                    port=5432,
                    database='airflow', 
                    user='airflow',
                    password='airflow'
                )
                cur = conn.cursor()
                
                # PostgreSQL ì‚½ì… (í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì €ì¥)
                insert_sql = """
                INSERT INTO news_articles
                (title, description, link, pub_date, keyword, collected_at)
                VALUES (%s, %s, %s, %s, %s, %s);
                """
                
                # í•œêµ­ ì‹œê°„ìœ¼ë¡œ collected_at ì„¤ì •
                collected_at = None
                if news_data.get('collected_at'):
                    try:
                        # ISO í˜•ì‹ì˜ í•œêµ­ ì‹œê°„ì„ íŒŒì‹±
                        collected_at = dt.fromisoformat(news_data['collected_at'].replace('Z', '+00:00'))
                        # ë§Œì•½ UTC ì‹œê°„ì´ë¼ë©´ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
                        if collected_at.tzinfo is None:
                            collected_at = KST.localize(collected_at)
                        else:
                            collected_at = collected_at.astimezone(KST)
                    except:
                        collected_at = dt.now(KST)  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ í•œêµ­ ì‹œê°„ ì‚¬ìš©
                else:
                    collected_at = dt.now(KST)  # collected_atì´ ì—†ìœ¼ë©´ í˜„ì¬ í•œêµ­ ì‹œê°„ ì‚¬ìš©
                
                values = (
                    news_data.get('title', '')[:500],
                    news_data.get('description', ''),
                    news_data.get('link', ''),
                    news_data.get('pub_date', ''),
                    news_data.get('keyword', ''),
                    collected_at
                )
                
                cur.execute(insert_sql, values)
                conn.commit()
                cur.close()
                conn.close()
                
                messages_processed += 1
                print(f"  âœ… DB ì €ì¥: {news_data.get('title', '')[:50]}...")
                
                # ì¶©ë¶„íˆ ì²˜ë¦¬í–ˆìœ¼ë©´ ì¢…ë£Œ
                if messages_processed >= 50:
                    break
                    
            except Exception as e:
                print(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
    
    except Exception as e:
        print(f"âŒ Kafka ì†Œë¹„ ì˜¤ë¥˜: {e}")
    
    finally:
        consumer.close()
    
    print(f"ğŸ‰ ì´ {messages_processed}ê°œ ë‰´ìŠ¤ë¥¼ PostgreSQLì— ì €ì¥ ì™„ë£Œ!")
    
    # ========================
    # 4ë‹¨ê³„: ê²°ê³¼ ìš”ì•½ (í•œêµ­ ì‹œê°„ìœ¼ë¡œ)
    # ========================
    result_summary = {
        'collected_count': len(collected_news),
        'stored_count': messages_processed,
        'keywords': keywords,
        'execution_time': datetime.now(KST).isoformat()  # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
    }
    
    print("ğŸ“Š ì‹¤í–‰ ê²°ê³¼:")
    print(f"  - ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {result_summary['collected_count']}ê°œ")
    print(f"  - ì €ì¥ëœ ë‰´ìŠ¤: {result_summary['stored_count']}ê°œ")
    print(f"  - ì²˜ë¦¬ëœ í‚¤ì›Œë“œ: {', '.join(keywords)}")
    print(f"  - ì‹¤í–‰ ì‹œê°„: {result_summary['execution_time']}")
    
    return result_summary

# Task ì •ì˜
integrated_task = PythonOperator(
    task_id='collect_and_store_news',
    python_callable=collect_and_store_news,
    dag=dag
)