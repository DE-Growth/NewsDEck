from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import json
from kafka import KafkaConsumer
from datetime import datetime as dt

# ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'seoyunjang',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# DAG ì •ì˜
dag = DAG(
    'news_etl_pipeline',
    default_args=default_args,
    description='ë‰´ìŠ¤ ë°ì´í„° ETL íŒŒì´í”„ë¼ì¸',
    schedule_interval=timedelta(minutes=30),
    catchup=False,
    tags=['news', 'etl', 'kafka']
)

def consume_kafka_messages():
    """Kafkaì—ì„œ ë©”ì‹œì§€ ì†Œë¹„ ë° PostgreSQL ì €ì¥"""
    
    # Kafka Consumer ì„¤ì •
    consumer = KafkaConsumer(
        'news-topic',
        bootstrap_servers=['kafka:9092'],
        value_deserializer=lambda m: json.loads(m.decode('utf-8')),
        group_id='airflow-consumer-group',
        auto_offset_reset='latest',
        enable_auto_commit=True,
        consumer_timeout_ms=30000
    )
    
    # PostgreSQL ì—°ê²°
    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    messages_processed = 0
    
    try:
        print("Kafka ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘...")
        
        for message in consumer:
            try:
                news_data = message.value
                
                if not news_data.get('title'):
                    continue
                
                # PostgreSQL ì‚½ì…
                insert_sql = """
                INSERT INTO news_articles 
                (title, description, link, pub_date, keyword, collected_at)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT DO NOTHING;
                """
                
                collected_at = None
                if news_data.get('collected_at'):
                    try:
                        collected_at = dt.fromisoformat(news_data['collected_at'].replace('Z', '+00:00'))
                    except:
                        collected_at = dt.now()
                
                values = (
                    news_data.get('title', '')[:500],
                    news_data.get('description', ''),
                    news_data.get('link', ''),
                    news_data.get('pub_date', ''),
                    news_data.get('keyword', ''),
                    collected_at
                )
                
                postgres_hook.run(insert_sql, parameters=values)
                messages_processed += 1
                
                print(f"âœ… ë©”ì‹œì§€ ì €ì¥: {news_data.get('title', '')[:50]}...")
                
                if messages_processed >= 100:
                    break
                    
            except Exception as e:
                print(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ Kafka ì†Œë¹„ ì˜¤ë¥˜: {e}")
    
    finally:
        consumer.close()
        print(f"ğŸ‰ ì´ {messages_processed}ê°œ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ")
    
    return messages_processed

# Task ì •ì˜
consume_task = PythonOperator(
    task_id='consume_kafka_messages',
    python_callable=consume_kafka_messages,
    dag=dag
)