### ✅ 완료된 작업

통합 개발 환경 구축 (**song-test/docker-compose.yml**)

Kafka, Zookeeper, PostgreSQL, Airflow 등 프로젝트에 필요한 모든 핵심 인프라를 Docker Compose로 한 번에 실행할 수 있도록 구성했습니다.

뉴스 수집기 개발 (producer.py) : 네이버 뉴스 API를 호출하여 지정된 키워드(IT, 주식 등)의 최신 기사를 수집하는 파이썬 스크립트를 완성했습니다.

**이슈** : 요약글만 가져오므로 원문을 수집하려면 별도의 코드가 필요함. 그리고 API 작동방식이 애초에 키워드를 검색해서 가져오는 것이라서 전체 뉴스를 수집하는 방식은 아님.

수집한 데이터를 정제하여 Kafka의 news-topic으로 전송하는 로직을 구현

인프라는 Docker로, 개발해야하는 뉴스 수집 코드는 로컬에서 실행하는 효율적인 개발 방식으로 채택함

producer.py 코드 내에서 Kafka 접속 주소를 간단히 주석 처리하여 로컬 개발 환경과 Docker 배포 환경에 대한 선택지 마련
데이터 전송 검증 완료

로컬에서 실행한 producer.py가 Docker 컨테이너로 실행 중인 Kafka에 데이터를 성공적으로 전송하는 것을 확인함

kafka-console-consumer 명령어를 통해, Producer가 꺼진 후에도 데이터가 Kafka에 안전하게 저장(영속성)되어 있음을 직접 확인함

### 실습 가이드

1. 프로젝트 파일 준비

Git에서 최신 프로젝트 파일을 내려받습니다. (docker-compose.yml, producer 폴더 등)

2. 환경 변수 설정

.env.example 파일을 복사하여 .env 파일을 만듭니다.

각자 발급받은 네이버 API 키를 .env 파일 안에 입력합니다.

3. 인프라 실행 (Docker)

터미널에서 아래 명령어로 Kafka, Postgres 등 핵심 인프라를 실행합니다.

docker-compose up -d

4. 로컬 개발 환경 설정

프로젝트 루트 폴더에 파이썬 가상 환경을 생성하고 활성화합니다.

- 가상환경 생성 (최초 1회)

```
python3 -m venv venv
```

- 가상환경 활성화

```
source venv/bin/activate
```

producer에 필요한 라이브러리를 설치합니다.

```
pip install -r producer/requirements.txt
```

5. 뉴스 수집기 실행 (로컬)

producer/producer.py 파일의 KAFKA_BROKER가 로컬용(localhost:9093)으로 설정되어 있는지 확인합니다.

터미널에서 스크립트를 직접 실행합니다.

```
python producer/producer.py
```

뉴스가 수집되고 Kafka로 전송되는 로그를 확인한 뒤, Ctrl + C로 스크립트를 중단합니다.

6. 데이터 확인 (Kafka)

아래 명령어로 Kafka 토픽에 데이터가 잘 저장되었는지 확인합니다.

```
docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic news-topic --from-beginning
```

다음에는 Airflow를 이용해 Kafka의 데이터를 DB로 적재하는 과정을 진행 예정

------
## 개념 정리

###  Kafka, 도대체 왜 쓰는 걸까? (초보자를 위한 개념 안내서)

방금 우리는 `producer.py`를 실행해서 네이버 뉴스를 수집하고, Kafka라는 곳에 데이터를 성공적으로 보냈습니다. 이 문서에서는 우리가 사용한 Kafka가 무엇이고, 관련된 용어들이 어떤 의미인지 알기 쉽게 설명합니다.

#### 🏢 카프카(Kafka)를 거대한 '데이터 물류 센터'라고 상상해 보세요.

실시간으로 쏟아지는 수많은 데이터(택배)들을 어딘가에 잠시 보관했다가, 필요한 곳(소비자)에 빠르고 안전하게 전달해주는 역할을 합니다. 우리 프로젝트에서는 계속해서 발생하는 '뉴스 기사'라는 데이터를 처리하기 위해 이 물류 센터를 사용하는 것입니다.


### 주요 용어와 우리 프로젝트에서의 역할

#### 1. 브로커 (Broker) - 물류 센터의 '창고 서버' 🏭

* **설명**: 데이터(메시지)가 실제로 저장되는 서버입니다. 우리 `docker-compose.yml` 파일에 있는 `kafka` 서비스가 바로 이 브로커 1대입니다.
* **역할**: Producer가 보낸 데이터를 받아서 디스크에 잘 저장하고, Consumer가 요청하면 데이터를 내어주는 핵심적인 역할을 합니다. 실제 대규모 서비스에서는 안정성과 성능을 위해 여러 대의 브로커를 함께 운영합니다.

#### 2. 주키퍼 (Zookeeper) - 물류 센터의 '총괄 매니저' 🧑‍💼

* **설명**: 여러 대의 브로커들을 관리하고 조율하는 지휘자입니다. `docker-compose.yml`에서 `zookeeper` 서비스가 이 역할을 합니다.
* **역할**: 어떤 브로커가 정상적으로 작동하는지 감시하고, 브로커들 사이의 정보를 조정합니다. "A 데이터는 1번 브로커가 담당해!", "2번 브로커가 고장났으니 작업을 중단시켜!" 와 같은 중요한 조율을 담당합니다.
    > (참고: 최신 Kafka는 주키퍼 없이 자체적으로 동작하는 모드(KRaft)도 지원하지만, 아직 많은 곳에서 주키퍼를 함께 사용합니다.)

#### 3. 토픽 (Topic) - 데이터의 '분류 라벨' 또는 '컨베이어 벨트' 🏷️

* **설명**: 데이터가 저장되는 일종의 카테고리입니다. 우리는 모든 뉴스 데이터를 `news-topic`이라는 이름의 토픽 하나에 저장했습니다.
* **역할**: 만약 우리가 주식 가격 데이터도 수집한다면 `stock-topic`이라는 다른 토픽을 만들어 데이터를 분리할 수 있습니다. 이렇게 토픽을 기준으로 데이터를 분류하면 Consumer들이 자신이 필요한 데이터만 골라서 가져갈 수 있습니다.

#### 4. 프로듀서 (Producer) - 데이터를 보내는 '생산자' 🚚

* **설명**: Kafka 브로커의 특정 토픽으로 데이터를 보내는 주체입니다. **우리가 만든 `producer.py`가 바로 프로듀서입니다.**
* **역할**: `producer.py`는 네이버에서 뉴스를 '생산'해서 `news-topic`이라는 '분류 라벨'을 붙여 물류 센터(브로커)로 보내는 역할만 합니다.

#### 5. 컨슈머 (Consumer) - 데이터를 가져가는 '소비자' 🙋‍♀️

* **설명**: Kafka 브로커의 특정 토픽에서 데이터를 가져와서 사용하는 주체입니다.
* **역할**: 우리 프로젝트에서는 **앞으로 만들 Airflow DAG**가 컨슈머 역할을 할 것입니다. 이 컨슈머는 `news-topic`에 쌓여있는 데이터를 주기적으로 가져와서 PostgreSQL 데이터베이스에 저장하는 일을 하게 됩니다.
* **우리가 사용한 `kafka-console-consumer.sh`**는 Kafka가 기본으로 제공하는 '테스트용 터미널 소비자'입니다.

---

### 우리가 사용한 코드와 명령어 다시 보기

이제 위 용어들을 우리가 실제 사용한 코드에 대입해 보면 훨씬 명확해집니다.

* **`KAFKA_BROKER = 'localhost:9093'` 또는 `'kafka:9092'`**
    * 이 코드는 프로듀서(`producer.py`)에게 데이터를 보낼 **물류 센터(브로커)의 주소**를 알려주는 것입니다. Kafka에서는 이 주소를 **부트스트랩 서버(Bootstrap Server)**라고 부릅니다. 프로듀서는 이 주소만 알면 전체 물류 시스템에 데이터를 보낼 수 있습니다.

* **`producer.send('news-topic', value=cleaned_item)`**
    * `producer.py`의 이 부분은 "내가 방금 만든 `cleaned_item`이라는 데이터(택배)를 `news-topic`이라는 이름의 컨베이어 벨트(토픽)에 올려줘!" 라는 의미입니다.

* **`docker-compose exec kafka kafka-console-consumer.sh --topic news-topic ...`**
    * 이 명령어는 "테스트용 소비자(컨슈머)를 실행해서 `news-topic` 컨베이어 벨트를 계속 지켜보다가, 데이터(택배)가 보이면 화면에 즉시 보여줘!" 라는 의미입니다.

---

### ✨ 핵심 요약: 그래서 왜 쓸까?

그럼 이렇게 복잡해 보이는 Kafka를 왜 쓸까요? 가장 큰 이유는 **'분리'와 '안정성'** 때문입니다.

1.  **느슨한 결합 (Decoupling)**: Producer는 데이터만 던져놓으면 끝입니다. 나중에 이 데이터를 DB에 저장하든, AI 분석을 하든 전혀 신경 쓸 필요가 없습니다. 덕분에 시스템을 유연하게 확장할 수 있습니다.
2.  **데이터 안정성**: Consumer(Airflow)가 잠시 멈추거나 문제가 생겨도, 데이터는 Kafka에 안전하게 보관됩니다. Consumer가 다시 정상 작동하면 그동안 쌓였던 데이터를 순서대로 처리할 수 있어 데이터 유실 위험이 크게 줄어듭니다.
3.  **대용량 처리**: Kafka는 태생부터 초당 수십, 수백만 건의 데이터를 처리하기 위해 만들어져서 성능이 매우 뛰어납니다.
